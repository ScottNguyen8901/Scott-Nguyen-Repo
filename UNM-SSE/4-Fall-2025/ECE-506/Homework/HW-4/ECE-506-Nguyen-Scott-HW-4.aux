\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {paragraph}{Discussion sessions.}{1}{section*.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Reference.}{1}{section*.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Code.}{1}{section*.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Coding examples (required).}{1}{section*.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Stepsize $\alpha ^*$ as a function of $(a, x_1)$ for $f(x)=a x^2$.}}{3}{figure.1}\protected@file@percent }
\newlabel{fig:1cii}{{1}{3}{Stepsize $\alpha ^*$ as a function of $(a, x_1)$ for $f(x)=a x^2$}{figure.1}{}}
\newlabel{lst:1e}{{1}{4}{Verification code for Problem 1(e)}{lstlisting.1}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {1}Verification code for Problem 1(e)}{4}{lstlisting.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Verification of steepest descent for $f(x)=a x^2$. The algorithm converges to $x^\ast =0$ in two steps, consistent with the analytical result derived in~1(c).}}{5}{figure.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Robustness:}{6}{section*.7}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Efficiency:}{6}{section*.8}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Accuracy:}{6}{section*.9}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Compact summary across problems, methods, and random initializations. Columns retained to support: \emph  {Robustness} (runs, convergence, iterations); \emph  {Efficiency (memory)} in terms of $n$; and \emph  {Accuracy-lite} via terminal errors.}}{7}{table.1}\protected@file@percent }
\newlabel{tab:compact_summary_all}{{1}{7}{Compact summary across problems, methods, and random initializations. Columns retained to support: \emph {Robustness} (runs, convergence, iterations); \emph {Efficiency (memory)} in terms of $n$; and \emph {Accuracy-lite} via terminal errors}{table.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Robustness across $N=100$ random initializations for each test function.}}{8}{figure.3}\protected@file@percent }
\newlabel{fig:robustness}{{3}{8}{Robustness across $N=100$ random initializations for each test function}{figure.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Efficiency per iteration for Run~1: function, gradient, and Hessian evaluations, and memory usage.}}{9}{figure.4}\protected@file@percent }
\newlabel{fig:eff}{{4}{9}{Efficiency per iteration for Run~1: function, gradient, and Hessian evaluations, and memory usage}{figure.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Rate diagnostics for Run~1 showing $\log (e_k)$, $r_1=e_{k+1}/e_k$, and $r_2=e_{k+1}/e_k^2$.}}{10}{figure.5}\protected@file@percent }
\newlabel{fig:accuracy}{{5}{10}{Rate diagnostics for Run~1 showing $\log (e_k)$, $r_1=e_{k+1}/e_k$, and $r_2=e_{k+1}/e_k^2$}{figure.5}{}}
\@writefile{toc}{\contentsline {paragraph}{1. Symbolic Gradients and Hessians (optional).}{12}{section*.12}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{2. Rates of convergence.}{12}{section*.13}\protected@file@percent }
\gdef \@abspage@last{12}
